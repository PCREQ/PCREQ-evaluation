SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  196.8134, NLL-Loss  196.8127, KL-Loss    0.3708, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  145.5826, NLL-Loss  145.5630, KL-Loss    8.9508, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  155.6370, NLL-Loss  155.5835, KL-Loss   21.6594, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  137.8621, NLL-Loss  137.7783, KL-Loss   29.9202, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  111.2753, NLL-Loss  111.1507, KL-Loss   39.2729, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  124.5356, NLL-Loss  124.3610, KL-Loss   48.6077, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss  139.5635, NLL-Loss  139.3275, KL-Loss   57.9760, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  126.0278, NLL-Loss  125.7561, KL-Loss   58.9337, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  119.3780, NLL-Loss  119.0484, KL-Loss   63.1378, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  109.0129, NLL-Loss  108.6477, KL-Loss   61.7840, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  122.3625, NLL-Loss  121.9242, KL-Loss   65.4850, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  137.4003, NLL-Loss  136.8909, KL-Loss   67.2273, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss  125.7235, NLL-Loss  125.1725, KL-Loss   64.2420, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss  103.0740, NLL-Loss  102.4653, KL-Loss   62.6965, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss  113.5635, NLL-Loss  112.8718, KL-Loss   62.9503, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss  110.0131, NLL-Loss  109.1986, KL-Loss   65.5221, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  100.3069, NLL-Loss   99.4213, KL-Loss   62.9721, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  113.1508, NLL-Loss  112.2063, KL-Loss   59.3805, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss   96.3323, NLL-Loss   95.2585, KL-Loss   59.7015, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss  105.4374, NLL-Loss  104.3530, KL-Loss   53.3335, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss  102.6100, NLL-Loss  101.3115, KL-Loss   56.5115, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  108.5558, NLL-Loss  107.1144, KL-Loss   55.5294, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  125.4755, NLL-Loss  123.9819, KL-Loss   50.9552, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss  105.3701, NLL-Loss  103.7359, KL-Loss   49.3934, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss  111.9820, NLL-Loss  110.1836, KL-Loss   48.1809, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss  109.6948, NLL-Loss  107.7246, KL-Loss   46.8128, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss  112.8276, NLL-Loss  110.6886, KL-Loss   45.1011, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss  100.1444, NLL-Loss   97.9045, KL-Loss   45.6825, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  119.9734
Model saved at bin/2025-Jun-01-06:23:00/E0.pytorch
VALID Batch 0000/105, Loss  123.8965, NLL-Loss  121.7790, KL-Loss   43.0837, KL-Weight  0.049
VALID Batch 0050/105, Loss  124.3151, NLL-Loss  121.9854, KL-Loss   47.4002, KL-Weight  0.049
VALID Batch 0100/105, Loss   90.2241, NLL-Loss   88.2374, KL-Loss   40.4233, KL-Weight  0.049
VALID Batch 0105/105, Loss   90.5908, NLL-Loss   88.3528, KL-Loss   45.5355, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  104.7902
