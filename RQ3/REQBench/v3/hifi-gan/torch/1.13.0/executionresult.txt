Initializing Training Process..
Batch size per GPU : 5
Generator(
  (conv_pre): Conv1d(80, 512, kernel_size=(7,), stride=(1,), padding=(3,))
  (ups): ModuleList(
    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))
    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))
    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))
    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))
  )
  (resblocks): ModuleList(
    (0): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (1): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
      )
    )
    (2): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
      )
    )
    (3): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (4): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
      )
    )
    (5): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
      )
    )
    (6): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (7): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
      )
    )
    (8): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
      )
    )
    (9): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (10): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
      )
    )
    (11): ResBlock1(
      (convs1): ModuleList(
        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
      )
      (convs2): ModuleList(
        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
      )
    )
  )
  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))
)
checkpoints directory :  cp_hifigan
Loading 'cp_hifigan/g_00000030'
Complete.
Loading 'cp_hifigan/do_00000030'
Complete.
Epoch: 1
Loading 'cp_hifigan/g_00000030'
Complete.
Loading 'cp_hifigan/do_00000030'
Complete.
Loading 'cp_hifigan/g_00000030'
Complete.
Loading 'cp_hifigan/do_00000030'
Complete.
/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/functional.py:633: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)
  normalized, onesided, return_complex)
/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/functional.py:633: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)
  normalized, onesided, return_complex)
/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/functional.py:633: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)
  normalized, onesided, return_complex)
/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/functional.py:633: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)
  normalized, onesided, return_complex)
/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/functional.py:633: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)
  normalized, onesided, return_complex)
/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/functional.py:633: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)
  normalized, onesided, return_complex)
Traceback (most recent call last):
  File "train.py", line 277, in <module>
    main()
  File "train.py", line 271, in main
    mp.spawn(train, nprocs=h.num_gpus, args=(a, h,))
  File "/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/lei/anaconda3/envs/py37-1/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/lei/compatibility_analysis/pytorch/1.4/hifi-gan/train.py", line 124, in train
    h.fmin, h.fmax_for_loss)
  File "/home/lei/compatibility_analysis/pytorch/1.4/hifi-gan/meldataset.py", line 69, in mel_spectrogram
    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)
RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`

