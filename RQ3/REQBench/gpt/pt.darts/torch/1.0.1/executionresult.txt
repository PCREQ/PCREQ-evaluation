05/28 10:29:09 AM | 
05/28 10:29:09 AM | Parameters:
05/28 10:29:09 AM | AUX_WEIGHT=0.4
05/28 10:29:09 AM | BATCH_SIZE=256
05/28 10:29:09 AM | CUTOUT_LENGTH=16
05/28 10:29:09 AM | DATA_PATH=./data/
05/28 10:29:09 AM | DATASET=cifar10
05/28 10:29:09 AM | DROP_PATH_PROB=0.2
05/28 10:29:09 AM | EPOCHS=1
05/28 10:29:09 AM | GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('skip_connect', 0)], [('sep_conv_3x3', 1), ('skip_connect', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
05/28 10:29:09 AM | GPUS=[0, 1, 2]
05/28 10:29:09 AM | GRAD_CLIP=5.0
05/28 10:29:09 AM | INIT_CHANNELS=36
05/28 10:29:09 AM | LAYERS=20
05/28 10:29:09 AM | LR=0.025
05/28 10:29:09 AM | MOMENTUM=0.9
05/28 10:29:09 AM | NAME=cifar10
05/28 10:29:09 AM | PATH=augments/cifar10
05/28 10:29:09 AM | PRINT_FREQ=200
05/28 10:29:09 AM | SEED=2
05/28 10:29:09 AM | WEIGHT_DECAY=0.0003
05/28 10:29:09 AM | WORKERS=4
05/28 10:29:09 AM | 
05/28 10:29:09 AM | Logger is set - training start
05/28 10:29:14 AM | Model size = 3.159 MB
05/28 10:29:14 AM | Epoch 0 LR 0.025
Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "augment.py", line 177, in <module>
    main()
  File "augment.py", line 75, in main
    train(train_loader, model, optimizer, criterion, epoch)
  File "augment.py", line 111, in train
    logits, aux_logits = model(X)
  File "/home/lei/anaconda3/envs/py37-6/lib/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lei/anaconda3/envs/py37-6/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 142, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/home/lei/anaconda3/envs/py37-6/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 147, in replicate
    return replicate(module, device_ids)
  File "/home/lei/anaconda3/envs/py37-6/lib/python3.7/site-packages/torch/nn/parallel/replicate.py", line 13, in replicate
    param_copies = Broadcast.apply(devices, *params)
  File "/home/lei/anaconda3/envs/py37-6/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 21, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/home/lei/anaconda3/envs/py37-6/lib/python3.7/site-packages/torch/cuda/comm.py", line 40, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
RuntimeError: all tensors must be on devices[0]
