05/28/2025 04:37:08 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='kaggle', do_data=False, do_lower_case=True, do_test=False, do_train=True, epochs=1, eval_batch_size=8, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='2', predict_checkpoints=0, resume_path='', save_best=True, seed=42, sorted=1, train_batch_size=8, train_max_seq_len=256, valid_size=0.2, warmup_proportion=0.1, weight_decay=0.01)
05/28/2025 04:37:08 - INFO - root -   Loading examples from cached file pybert/dataset/cached_train_examples_bert
05/28/2025 04:37:09 - INFO - root -   Loading features from cached file pybert/dataset/cached_train_features_256_bert
05/28/2025 04:37:15 - INFO - root -   sorted data by th length of input
05/28/2025 04:37:17 - INFO - root -   Loading examples from cached file pybert/dataset/cached_valid_examples_bert
05/28/2025 04:37:17 - INFO - root -   Loading features from cached file pybert/dataset/cached_valid_features_256_bert
05/28/2025 04:37:19 - INFO - root -   initializing model
05/28/2025 04:37:19 - INFO - transformers.configuration_utils -   loading configuration file pybert/pretrain/bert/base-uncased/config.json
05/28/2025 04:37:19 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2025 04:37:19 - INFO - transformers.modeling_utils -   loading weights file pybert/pretrain/bert/base-uncased/pytorch_model.bin
05/28/2025 04:37:22 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at pybert/pretrain/bert/base-uncased were not used when initializing BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForMultiLable from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForMultiLable from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
05/28/2025 04:37:22 - WARNING - transformers.modeling_utils -   Some weights of BertForMultiLable were not initialized from the model checkpoint at pybert/pretrain/bert/base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/28/2025 04:37:22 - INFO - root -   initializing callbacks
05/28/2025 04:37:22 - INFO - root -   ***** Running training *****
05/28/2025 04:37:22 - INFO - root -     Num examples = 127657
05/28/2025 04:37:22 - INFO - root -     Num Epochs = 1
05/28/2025 04:37:22 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8
05/28/2025 04:37:22 - INFO - root -     Gradient Accumulation steps = 1
05/28/2025 04:37:22 - INFO - root -     Total optimization steps = 15958
05/28/2025 04:37:25 - INFO - root -   Epoch 1/1
[Training] 1/15958 [..............................] - ETA: 1:27:25  accuracy: 0.5625 - loss: 0.6664 [Training] 2/15958 [..............................] - ETA: 1:06:25  accuracy: 0.5625 - loss: 0.6796 [Training] 3/15958 [..............................] - ETA: 59:31  accuracy: 0.5208 - loss: 0.6737 [Training] 4/15958 [..............................] - ETA: 56:05  accuracy: 0.5833 - loss: 0.6782 [Training] 5/15958 [..............................] - ETA: 54:02  accuracy: 0.6458 - loss: 0.6671 [Training] 6/15958 [..............................] - ETA: 52:39  accuracy: 0.6458 - loss: 0.6645 [Training] 7/15958 [..............................] - ETA: 51:42  accuracy: 0.6042 - loss: 0.6716 [Training] 8/15958 [..............................] - ETA: 50:37  accuracy: 0.6250 - loss: 0.6694 [Training] 9/15958 [..............................] - ETA: 50:05  accuracy: 0.5625 - loss: 0.6744 [Training] 10/15958 [..............................] - ETA: 49:40  accuracy: 0.5625 - loss: 0.6764 [Training] 11/15958 [..............................] - ETA: 49:19  accuracy: 0.5625 - loss: 0.6586 [Training] 12/15958 [..............................] - ETA: 49:03  accuracy: 0.5625 - loss: 0.6785 [Training] 13/15958 [..............................] - ETA: 48:48  accuracy: 0.5833 - loss: 0.6767 [Training] 14/15958 [..............................] - ETA: 48:35  accuracy: 0.5833 - loss: 0.6727 [Training] 15/15958 [..............................] - ETA: 48:24  accuracy: 0.5208 - loss: 0.6940 [Training] 16/15958 [..............................] - ETA: 48:14  accuracy: 0.6042 - loss: 0.6651 [Training] 17/15958 [..............................] - ETA: 48:05  accuracy: 0.5833 - loss: 0.6632 [Training] 18/15958 [..............................] - ETA: 48:00  accuracy: 0.5625 - loss: 0.6470 [Training] 19/15958 [..............................] - ETA: 47:52  accuracy: 0.6667 - loss: 0.6680 [Training] 20/15958 [..............................] - ETA: 47:46  accuracy: 0.6042 - loss: 0.6906 [Training] 21/15958 [..............................] - ETA: 47:40  accuracy: 0.5833 - loss: 0.6560 [Training] 22/15958 [..............................] - ETA: 47:35  accuracy: 0.6042 - loss: 0.6573 [Training] 23/15958 [..............................] - ETA: 47:31  accuracy: 0.5417 - loss: 0.6820 [Training] 24/15958 [..............................] - ETA: 47:27  accuracy: 0.6042 - loss: 0.6663 [Training] 25/15958 [..............................] - ETA: 47:23  accuracy: 0.6042 - loss: 0.6830 [Training] 26/15958 [..............................] - ETA: 47:19  accuracy: 0.6042 - loss: 0.6660 [Training] 27/15958 [..............................] - ETA: 47:14  accuracy: 0.5208 - loss: 0.6576 [Training] 28/15958 [..............................] - ETA: 47:11  accuracy: 0.5417 - loss: 0.6982 [Training] 29/15958 [..............................] - ETA: 47:08  accuracy: 0.5833 - loss: 0.6806 [Training] 30/15958 [..............................] - ETA: 47:05  accuracy: 0.6458 - loss: 0.6661 [Training] 31/15958 [..............................] - ETA: 47:03  accuracy: 0.6667 - loss: 0.6575 [Training] 32/15958 [..............................] - ETA: 47:00  accuracy: 0.6250 - loss: 0.6638 [Training] 33/15958 [..............................] - ETA: 46:58  accuracy: 0.5625 - loss: 0.6726 [Training] 34/15958 [..............................] - ETA: 46:56  accuracy: 0.6042 - loss: 0.6574 [Training] 35/15958 [..............................] - ETA: 46:53  accuracy: 0.6458 - loss: 0.6626 [Training] 36/15958 [..............................] - ETA: 46:51  accuracy: 0.6458 - loss: 0.6499 [Training] 37/15958 [..............................] - ETA: 46:49  accuracy: 0.6042 - loss: 0.6491 [Training] 38/15958 [..............................] - ETA: 46:48  accuracy: 0.5625 - loss: 0.6623 [Training] 39/15958 [..............................] - ETA: 46:46  accuracy: 0.6250 - loss: 0.6490 [Training] 40/15958 [..............................] - ETA: 46:45  accuracy: 0.6042 - loss: 0.6579 [Training] 41/15958 [..............................] - ETA: 46:43  accuracy: 0.6042 - loss: 0.6498 [Training] 42/15958 [..............................] - ETA: 46:42  accuracy: 0.7083 - loss: 0.6438 [Training] 43/15958 [..............................] - ETA: 46:41  accuracy: 0.6250 - loss: 0.6612 [Training] 44/15958 [..............................] - ETA: 46:39  accuracy: 0.6250 - loss: 0.6456 [Training] 45/15958 [..............................] - ETA: 46:38  accuracy: 0.5833 - loss: 0.6572 [Training] 46/15958 [..............................] - ETA: 46:36  accuracy: 0.6042 - loss: 0.6467 [Training] 47/15958 [..............................] - ETA: 46:35  accuracy: 0.5625 - loss: 0.6554 [Training] 48/15958 [..............................] - ETA: 46:34  accuracy: 0.6250 - loss: 0.6391 [Training] 49/15958 [..............................] - ETA: 46:33  accuracy: 0.6042 - loss: 0.6544 [Training] 50/15958 [..............................] - ETA: 46:32  accuracy: 0.5625 - loss: 0.6703 [Training] 51/15958 [..............................] - ETA: 46:30  accuracy: 0.5833 - loss: 0.6609 [Training] 52/15958 [..............................] - ETA: 46:29  accuracy: 0.6458 - loss: 0.6414 [Training] 53/15958 [..............................] - ETA: 46:28  accuracy: 0.6250 - loss: 0.6545 [Training] 54/15958 [..............................] - ETA: 46:27  accuracy: 0.6250 - loss: 0.6497 [Training] 55/15958 [..............................] - ETA: 46:26  accuracy: 0.6667 - loss: 0.6393 [Training] 56/15958 [..............................] - ETA: 46:25  accuracy: 0.6667 - loss: 0.6302 [Training] 57/15958 [..............................] - ETA: 46:25  accuracy: 0.6042 - loss: 0.6361 [Training] 58/15958 [..............................] - ETA: 46:24  accuracy: 0.6250 - loss: 0.6301 [Training] 59/15958 [..............................] - ETA: 46:25  accuracy: 0.6250 - loss: 0.6203 [Training] 60/15958 [..............................] - ETA: 46:24  accuracy: 0.6458 - loss: 0.6241 [Training] 61/15958 [..............................] - ETA: 46:24  accuracy: 0.6458 - loss: 0.6549 [Training] 62/15958 [..............................] - ETA: 46:23  accuracy: 0.6667 - loss: 0.6289 [Training] 63/15958 [..............................] - ETA: 46:22  accuracy: 0.6667 - loss: 0.6297 [Training] 64/15958 [..............................] - ETA: 46:21  accuracy: 0.6250 - loss: 0.6356 [Training] 65/15958 [..............................] - ETA: 46:21  accuracy: 0.6250 - loss: 0.6196 [Training] 66/15958 [..............................] - ETA: 46:20  accuracy: 0.6875 - loss: 0.6094 [Training] 67/15958 [..............................] - ETA: 46:20  accuracy: 0.6667 - loss: 0.6155 [Training] 68/15958 [..............................] - ETA: 46:19  accuracy: 0.7292 - loss: 0.6177 [Training] 69/15958 [..............................] - ETA: 46:18  accuracy: 0.7500 - loss: 0.6123 [Training] 70/15958 [..............................] - ETA: 46:17  accuracy: 0.6458 - loss: 0.6151 [Training] 71/15958 [..............................] - ETA: 46:17  accuracy: 0.7292 - loss: 0.6136 [Training] 72/15958 [..............................] - ETA: 46:16  accuracy: 0.7083 - loss: 0.6262 [Training] 73/15958 [..............................] - ETA: 46:15  accuracy: 0.7500 - loss: 0.6253 [Training] 74/15958 [..............................] - ETA: 46:15  accuracy: 0.7500 - loss: 0.6097 [Training] 75/15958 [..............................] - ETA: 46:14  accuracy: 0.7083 - loss: 0.6045 [Training] 76/15958 [..............................] - ETA: 46:13  accuracy: 0.6875 - loss: 0.5986 [Training] 77/15958 [..............................] - ETA: 46:13  accuracy: 0.7708 - loss: 0.5905 [Training] 78/15958 [..............................] - ETA: 46:12  accuracy: 0.8125 - loss: 0.5981 [Training] 79/15958 [..............................] - ETA: 46:11  accuracy: 0.7083 - loss: 0.5904 [Training] 80/15958 [..............................] - ETA: 46:11  accuracy: 0.6875 - loss: 0.6009 [Training] 81/15958 [..............................] - ETA: 46:10  accuracy: 0.7083 - loss: 0.6055 [Training] 82/15958 [..............................] - ETA: 46:08  accuracy: 0.7292 - loss: 0.5939 [Training] 83/15958 [..............................] - ETA: 46:07  accuracy: 0.6875 - loss: 0.6041 [Training] 84/15958 [..............................] - ETA: 46:07  accuracy: 0.7083 - loss: 0.5976 [Training] 85/15958 [..............................] - ETA: 46:06  accuracy: 0.7292 - loss: 0.6041 [Training] 86/15958 [..............................] - ETA: 46:06  accuracy: 0.7083 - loss: 0.6128 [Training] 87/15958 [..............................] - ETA: 46:05  accuracy: 0.7708 - loss: 0.5928 [Training] 88/15958 [..............................] - ETA: 46:05  accuracy: 0.7917 - loss: 0.5666 [Training] 89/15958 [..............................] - ETA: 46:05  accuracy: 0.7708 - loss: 0.5846 [Training] 90/15958 [..............................] - ETA: 46:04  accuracy: 0.7500 - loss: 0.6012 [Training] 91/15958 [..............................] - ETA: 46:04  accuracy: 0.8125 - loss: 0.5849 [Training] 92/15958 [..............................] - ETA: 46:03  accuracy: 0.7292 - loss: 0.5766 [Training] 93/15958 [..............................] - ETA: 46:03  accuracy: 0.7917 - loss: 0.5641 [Training] 94/15958 [..............................] - ETA: 46:02  accuracy: 0.8333 - loss: 0.5605 [Training] 95/15958 [..............................] - ETA: 46:02  accuracy: 0.7083 - loss: 0.5926 [Training] 96/15958 [..............................] - ETA: 46:01  accuracy: 0.7917 - loss: 0.5621 [Training] 97/15958 [..............................] - ETA: 46:01  accuracy: 0.7708 - loss: 0.5865 [Training] 98/15958 [..............................] - ETA: 46:00  accuracy: 0.8333 - loss: 0.5505 [Training] 99/15958 [..............................] - ETA: 46:00  accuracy: 0.7708 - loss: 0.5648 [Training] 100/15958 [..............................] - ETA: 46:00  accuracy: 0.7708 - loss: 0.5835 [Training] 101/15958 [..............................] - ETA: 45:59  accuracy: 0.8333 - loss: 0.5577 
------------- train result --------------
label:toxic - auc: 0.5628
label:severe_toxic - auc: 0.6236
label:obscene - auc: 0.5525
label:threat - auc: 0.3408
label:insult - auc: 0.4853
label:identity_hate - auc: 0.5869
[Evaluating] 1/3990 [..............................] - ETA: 2:56[Evaluating] 2/3990 [..............................] - ETA: 3:03[Evaluating] 3/3990 [..............................] - ETA: 2:34[Evaluating] 4/3990 [..............................] - ETA: 2:43[Evaluating] 5/3990 [..............................] - ETA: 2:48[Evaluating] 6/3990 [..............................] - ETA: 2:50[Evaluating] 7/3990 [..............................] - ETA: 2:51[Evaluating] 8/3990 [..............................] - ETA: 2:48[Evaluating] 9/3990 [..............................] - ETA: 2:47[Evaluating] 10/3990 [..............................] - ETA: 2:48[Evaluating] 11/3990 [..............................] - ETA: 2:50[Evaluating] 12/3990 [..............................] - ETA: 2:52[Evaluating] 13/3990 [..............................] - ETA: 2:45[Evaluating] 14/3990 [..............................] - ETA: 2:46[Evaluating] 15/3990 [..............................] - ETA: 2:43[Evaluating] 16/3990 [..............................] - ETA: 2:37[Evaluating] 17/3990 [..............................] - ETA: 2:39[Evaluating] 18/3990 [..............................] - ETA: 2:34[Evaluating] 19/3990 [..............................] - ETA: 2:36[Evaluating] 20/3990 [..............................] - ETA: 2:37[Evaluating] 21/3990 [..............................] - ETA: 2:37[Evaluating] 22/3990 [..............................] - ETA: 2:39[Evaluating] 23/3990 [..............................] - ETA: 2:35[Evaluating] 24/3990 [..............................] - ETA: 2:36[Evaluating] 25/3990 [..............................] - ETA: 2:34[Evaluating] 26/3990 [..............................] - ETA: 2:34[Evaluating] 27/3990 [..............................] - ETA: 2:35[Evaluating] 28/3990 [..............................] - ETA: 2:33[Evaluating] 29/3990 [..............................] - ETA: 2:34[Evaluating] 30/3990 [..............................] - ETA: 2:32[Evaluating] 31/3990 [..............................] - ETA: 2:30[Evaluating] 32/3990 [..............................] - ETA: 2:28[Evaluating] 33/3990 [..............................] - ETA: 2:29[Evaluating] 34/3990 [..............................] - ETA: 2:27[Evaluating] 35/3990 [..............................] - ETA: 2:28[Evaluating] 36/3990 [..............................] - ETA: 2:27[Evaluating] 37/3990 [..............................] - ETA: 2:25[Evaluating] 38/3990 [..............................] - ETA: 2:26[Evaluating] 39/3990 [..............................] - ETA: 2:26[Evaluating] 40/3990 [..............................] - ETA: 2:24[Evaluating] 41/3990 [..............................] - ETA: 2:22[Evaluating] 42/3990 [..............................] - ETA: 2:22[Evaluating] 43/3990 [..............................] - ETA: 2:20[Evaluating] 44/3990 [..............................] - ETA: 2:21[Evaluating] 45/3990 [..............................] - ETA: 2:22[Evaluating] 46/3990 [..............................] - ETA: 2:21[Evaluating] 47/3990 [..............................] - ETA: 2:20[Evaluating] 48/3990 [..............................] - ETA: 2:19[Evaluating] 49/3990 [..............................] - ETA: 2:18[Evaluating] 50/3990 [..............................] - ETA: 2:18[Evaluating] 51/3990 [..............................] - ETA: 2:18[Evaluating] 52/3990 [..............................] - ETA: 2:17[Evaluating] 53/3990 [..............................] - ETA: 2:18[Evaluating] 54/3990 [..............................] - ETA: 2:19[Evaluating] 55/3990 [..............................] - ETA: 2:20[Evaluating] 56/3990 [..............................] - ETA: 2:19[Evaluating] 57/3990 [..............................] - ETA: 2:20[Evaluating] 58/3990 [..............................] - ETA: 2:20[Evaluating] 59/3990 [..............................] - ETA: 2:20[Evaluating] 60/3990 [..............................] - ETA: 2:21[Evaluating] 61/3990 [..............................] - ETA: 2:20[Evaluating] 62/3990 [..............................] - ETA: 2:21[Evaluating] 63/3990 [..............................] - ETA: 2:20[Evaluating] 64/3990 [..............................] - ETA: 2:20[Evaluating] 65/3990 [..............................] - ETA: 2:19[Evaluating] 66/3990 [..............................] - ETA: 2:20[Evaluating] 67/3990 [..............................] - ETA: 2:19[Evaluating] 68/3990 [..............................] - ETA: 2:20[Evaluating] 69/3990 [..............................] - ETA: 2:20[Evaluating] 70/3990 [..............................] - ETA: 2:21[Evaluating] 71/3990 [..............................] - ETA: 2:21[Evaluating] 72/3990 [..............................] - ETA: 2:22[Evaluating] 73/3990 [..............................] - ETA: 2:22[Evaluating] 74/3990 [..............................] - ETA: 2:23[Evaluating] 75/3990 [..............................] - ETA: 2:23[Evaluating] 76/3990 [..............................] - ETA: 2:23[Evaluating] 77/3990 [..............................] - ETA: 2:23[Evaluating] 78/3990 [..............................] - ETA: 2:24[Evaluating] 79/3990 [..............................] - ETA: 2:24[Evaluating] 80/3990 [..............................] - ETA: 2:24[Evaluating] 81/3990 [..............................] - ETA: 2:24[Evaluating] 82/3990 [..............................] - ETA: 2:23[Evaluating] 83/3990 [..............................] - ETA: 2:24[Evaluating] 84/3990 [..............................] - ETA: 2:23[Evaluating] 85/3990 [..............................] - ETA: 2:23[Evaluating] 86/3990 [..............................] - ETA: 2:23[Evaluating] 87/3990 [..............................] - ETA: 2:23[Evaluating] 88/3990 [..............................] - ETA: 2:24[Evaluating] 89/3990 [..............................] - ETA: 2:23[Evaluating] 90/3990 [..............................] - ETA: 2:23[Evaluating] 91/3990 [..............................] - ETA: 2:22[Evaluating] 92/3990 [..............................] - ETA: 2:22[Evaluating] 93/3990 [..............................] - ETA: 2:2205/28/2025 04:37:48 - INFO - root -   
Epoch: 1 -  loss: 0.6355 - auc: 0.5479 - valid_loss: 0.6022 - valid_auc: 0.5627 
05/28/2025 04:37:48 - INFO - root -   
Epoch 1: valid_loss improved from inf to 0.60222
05/28/2025 04:37:48 - INFO - transformers.configuration_utils -   Configuration saved in pybert/output/checkpoints/bert/config.json
05/28/2025 04:38:01 - INFO - transformers.modeling_utils -   Model weights saved in pybert/output/checkpoints/bert/pytorch_model.bin
[Evaluating] 94/3990 [..............................] - ETA: 2:22[Evaluating] 95/3990 [..............................] - ETA: 2:22[Evaluating] 96/3990 [..............................] - ETA: 2:22[Evaluating] 97/3990 [..............................] - ETA: 2:22[Evaluating] 98/3990 [..............................] - ETA: 2:22[Evaluating] 99/3990 [..............................] - ETA: 2:22[Evaluating] 100/3990 [..............................] - ETA: 2:23[Evaluating] 101/3990 [..............................] - ETA: 2:23------------- valid result --------------
label:toxic - auc: 0.5299
label:severe_toxic - auc: 0.4869
label:obscene - auc: 0.6401
label:threat - auc: 0.6760
label:insult - auc: 0.5820
label:identity_hate - auc: 0.6096
