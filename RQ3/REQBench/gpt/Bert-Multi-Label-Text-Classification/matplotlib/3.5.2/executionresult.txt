05/28/2025 10:16:05 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='kaggle', do_data=False, do_lower_case=True, do_test=False, do_train=True, epochs=1, eval_batch_size=8, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='2', predict_checkpoints=0, resume_path='', save_best=True, seed=42, sorted=1, train_batch_size=8, train_max_seq_len=256, valid_size=0.2, warmup_proportion=0.1, weight_decay=0.01)
05/28/2025 10:16:05 - INFO - root -   Loading examples from cached file pybert/dataset/cached_train_examples_bert
05/28/2025 10:16:06 - INFO - root -   Loading features from cached file pybert/dataset/cached_train_features_256_bert
05/28/2025 10:16:12 - INFO - root -   sorted data by th length of input
05/28/2025 10:16:14 - INFO - root -   Loading examples from cached file pybert/dataset/cached_valid_examples_bert
05/28/2025 10:16:14 - INFO - root -   Loading features from cached file pybert/dataset/cached_valid_features_256_bert
05/28/2025 10:16:16 - INFO - root -   initializing model
05/28/2025 10:16:17 - INFO - transformers.configuration_utils -   loading configuration file pybert/pretrain/bert/base-uncased/config.json
05/28/2025 10:16:17 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 6,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

05/28/2025 10:16:17 - INFO - transformers.modeling_utils -   loading weights file pybert/pretrain/bert/base-uncased/pytorch_model.bin
05/28/2025 10:16:19 - INFO - transformers.modeling_utils -   Weights of BertForMultiLable not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2025 10:16:19 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/28/2025 10:16:19 - INFO - root -   initializing callbacks
05/28/2025 10:16:19 - INFO - root -   ***** Running training *****
05/28/2025 10:16:19 - INFO - root -     Num examples = 127657
05/28/2025 10:16:19 - INFO - root -     Num Epochs = 1
05/28/2025 10:16:19 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8
05/28/2025 10:16:19 - INFO - root -     Gradient Accumulation steps = 1
05/28/2025 10:16:19 - INFO - root -     Total optimization steps = 15958
05/28/2025 10:16:23 - INFO - root -   Epoch 1/1
[Training] 1/15958 [..............................] - ETA: 1:05:27  accuracy: 0.4792 - loss: 0.6911 [Training] 2/15958 [..............................] - ETA: 57:30  accuracy: 0.5208 - loss: 0.6710 [Training] 3/15958 [..............................] - ETA: 55:34  accuracy: 0.5000 - loss: 0.6836 [Training] 4/15958 [..............................] - ETA: 54:31  accuracy: 0.5417 - loss: 0.6784 [Training] 5/15958 [..............................] - ETA: 54:01  accuracy: 0.5625 - loss: 0.6801 [Training] 6/15958 [..............................] - ETA: 53:46  accuracy: 0.4583 - loss: 0.6917 [Training] 7/15958 [..............................] - ETA: 53:34  accuracy: 0.6458 - loss: 0.6640 [Training] 8/15958 [..............................] - ETA: 53:22  accuracy: 0.5417 - loss: 0.6752 [Training] 9/15958 [..............................] - ETA: 53:11  accuracy: 0.6250 - loss: 0.6700 [Training] 10/15958 [..............................] - ETA: 53:03  accuracy: 0.5833 - loss: 0.6789 [Training] 11/15958 [..............................] - ETA: 52:58  accuracy: 0.5000 - loss: 0.6778 [Training] 12/15958 [..............................] - ETA: 52:45  accuracy: 0.5833 - loss: 0.6766 [Training] 13/15958 [..............................] - ETA: 52:50  accuracy: 0.5625 - loss: 0.6661 [Training] 14/15958 [..............................] - ETA: 52:47  accuracy: 0.5625 - loss: 0.6846 [Training] 15/15958 [..............................] - ETA: 52:48  accuracy: 0.4583 - loss: 0.7024 [Training] 16/15958 [..............................] - ETA: 52:45  accuracy: 0.5000 - loss: 0.6848 [Training] 17/15958 [..............................] - ETA: 52:42  accuracy: 0.5625 - loss: 0.6729 [Training] 18/15958 [..............................] - ETA: 52:38  accuracy: 0.5625 - loss: 0.6569 [Training] 19/15958 [..............................] - ETA: 52:37  accuracy: 0.5625 - loss: 0.6694 [Training] 20/15958 [..............................] - ETA: 52:37  accuracy: 0.5208 - loss: 0.6982 [Training] 21/15958 [..............................] - ETA: 52:36  accuracy: 0.5833 - loss: 0.6670 [Training] 22/15958 [..............................] - ETA: 52:35  accuracy: 0.6042 - loss: 0.6647 [Training] 23/15958 [..............................] - ETA: 52:37  accuracy: 0.4792 - loss: 0.6767 [Training] 24/15958 [..............................] - ETA: 52:38  accuracy: 0.5625 - loss: 0.6673 [Training] 25/15958 [..............................] - ETA: 52:38  accuracy: 0.6250 - loss: 0.6717 [Training] 26/15958 [..............................] - ETA: 52:36  accuracy: 0.6250 - loss: 0.6616 [Training] 27/15958 [..............................] - ETA: 52:34  accuracy: 0.6042 - loss: 0.6595 [Training] 28/15958 [..............................] - ETA: 52:32  accuracy: 0.5625 - loss: 0.6905 [Training] 29/15958 [..............................] - ETA: 52:32  accuracy: 0.5417 - loss: 0.6871 [Training] 30/15958 [..............................] - ETA: 52:29  accuracy: 0.5417 - loss: 0.6748 [Training] 31/15958 [..............................] - ETA: 52:29  accuracy: 0.5208 - loss: 0.6834 [Training] 32/15958 [..............................] - ETA: 52:28  accuracy: 0.5417 - loss: 0.6609 [Training] 33/15958 [..............................] - ETA: 52:26  accuracy: 0.5625 - loss: 0.6705 [Training] 34/15958 [..............................] - ETA: 52:25  accuracy: 0.5625 - loss: 0.6789 [Training] 35/15958 [..............................] - ETA: 52:26  accuracy: 0.5625 - loss: 0.6476 [Training] 36/15958 [..............................] - ETA: 52:28  accuracy: 0.6250 - loss: 0.6586 [Training] 37/15958 [..............................] - ETA: 52:28  accuracy: 0.6250 - loss: 0.6465 [Training] 38/15958 [..............................] - ETA: 52:27  accuracy: 0.5833 - loss: 0.6559 [Training] 39/15958 [..............................] - ETA: 52:25  accuracy: 0.6458 - loss: 0.6475 [Training] 40/15958 [..............................] - ETA: 52:26  accuracy: 0.5625 - loss: 0.6533 [Training] 41/15958 [..............................] - ETA: 52:25  accuracy: 0.6042 - loss: 0.6595 [Training] 42/15958 [..............................] - ETA: 52:25  accuracy: 0.7083 - loss: 0.6350 [Training] 43/15958 [..............................] - ETA: 52:24  accuracy: 0.5833 - loss: 0.6697 [Training] 44/15958 [..............................] - ETA: 52:23  accuracy: 0.5625 - loss: 0.6467 [Training] 45/15958 [..............................] - ETA: 52:22  accuracy: 0.6458 - loss: 0.6489 [Training] 46/15958 [..............................] - ETA: 52:23  accuracy: 0.6458 - loss: 0.6527 [Training] 47/15958 [..............................] - ETA: 52:24  accuracy: 0.6458 - loss: 0.6483 [Training] 48/15958 [..............................] - ETA: 52:23  accuracy: 0.6250 - loss: 0.6472 [Training] 49/15958 [..............................] - ETA: 52:23  accuracy: 0.5417 - loss: 0.6672 [Training] 50/15958 [..............................] - ETA: 52:23  accuracy: 0.5208 - loss: 0.6689 [Training] 51/15958 [..............................] - ETA: 52:22  accuracy: 0.6667 - loss: 0.6361 [Training] 52/15958 [..............................] - ETA: 52:23  accuracy: 0.6667 - loss: 0.6354 [Training] 53/15958 [..............................] - ETA: 52:22  accuracy: 0.5417 - loss: 0.6539 [Training] 54/15958 [..............................] - ETA: 52:21  accuracy: 0.5833 - loss: 0.6611 [Training] 55/15958 [..............................] - ETA: 52:22  accuracy: 0.5625 - loss: 0.6449 [Training] 56/15958 [..............................] - ETA: 52:21  accuracy: 0.6042 - loss: 0.6497 [Training] 57/15958 [..............................] - ETA: 52:20  accuracy: 0.6667 - loss: 0.6354 [Training] 58/15958 [..............................] - ETA: 52:20  accuracy: 0.6250 - loss: 0.6467 [Training] 59/15958 [..............................] - ETA: 52:20  accuracy: 0.7292 - loss: 0.6158 [Training] 60/15958 [..............................] - ETA: 52:19  accuracy: 0.6667 - loss: 0.6296 [Training] 61/15958 [..............................] - ETA: 52:18  accuracy: 0.6875 - loss: 0.6571 [Training] 62/15958 [..............................] - ETA: 52:17  accuracy: 0.7292 - loss: 0.6263 [Training] 63/15958 [..............................] - ETA: 52:17  accuracy: 0.7500 - loss: 0.6117 [Training] 64/15958 [..............................] - ETA: 52:18  accuracy: 0.6667 - loss: 0.6190 [Training] 65/15958 [..............................] - ETA: 52:18  accuracy: 0.6250 - loss: 0.6247 [Training] 66/15958 [..............................] - ETA: 52:17  accuracy: 0.7708 - loss: 0.6198 [Training] 67/15958 [..............................] - ETA: 52:16  accuracy: 0.7292 - loss: 0.6129 [Training] 68/15958 [..............................] - ETA: 52:16  accuracy: 0.6667 - loss: 0.6170 [Training] 69/15958 [..............................] - ETA: 52:16  accuracy: 0.6875 - loss: 0.6304 [Training] 70/15958 [..............................] - ETA: 52:16  accuracy: 0.6250 - loss: 0.6181 [Training] 71/15958 [..............................] - ETA: 52:16  accuracy: 0.7500 - loss: 0.6146 [Training] 72/15958 [..............................] - ETA: 52:16  accuracy: 0.6667 - loss: 0.6377 [Training] 73/15958 [..............................] - ETA: 52:16  accuracy: 0.7500 - loss: 0.6322 [Training] 74/15958 [..............................] - ETA: 52:16  accuracy: 0.6875 - loss: 0.6183 [Training] 75/15958 [..............................] - ETA: 52:16  accuracy: 0.7500 - loss: 0.5930 [Training] 76/15958 [..............................] - ETA: 52:16  accuracy: 0.7083 - loss: 0.6059 [Training] 77/15958 [..............................] - ETA: 52:16  accuracy: 0.7708 - loss: 0.5893 [Training] 78/15958 [..............................] - ETA: 52:16  accuracy: 0.6875 - loss: 0.6119 [Training] 79/15958 [..............................] - ETA: 52:16  accuracy: 0.7292 - loss: 0.5895 [Training] 80/15958 [..............................] - ETA: 52:17  accuracy: 0.8125 - loss: 0.5830 [Training] 81/15958 [..............................] - ETA: 52:17  accuracy: 0.6667 - loss: 0.6097 [Training] 82/15958 [..............................] - ETA: 52:17  accuracy: 0.7292 - loss: 0.6086 [Training] 83/15958 [..............................] - ETA: 52:17  accuracy: 0.6667 - loss: 0.6141 [Training] 84/15958 [..............................] - ETA: 52:17  accuracy: 0.6875 - loss: 0.5879 [Training] 85/15958 [..............................] - ETA: 52:17  accuracy: 0.7083 - loss: 0.6097 [Training] 86/15958 [..............................] - ETA: 52:17  accuracy: 0.7500 - loss: 0.5940 [Training] 87/15958 [..............................] - ETA: 52:17  accuracy: 0.6667 - loss: 0.6106 [Training] 88/15958 [..............................] - ETA: 52:17  accuracy: 0.8333 - loss: 0.5729 [Training] 89/15958 [..............................] - ETA: 52:21  accuracy: 0.7500 - loss: 0.5690 [Training] 90/15958 [..............................] - ETA: 52:23  accuracy: 0.8333 - loss: 0.5995 [Training] 91/15958 [..............................] - ETA: 52:23  accuracy: 0.7917 - loss: 0.5678 [Training] 92/15958 [..............................] - ETA: 52:22  accuracy: 0.8542 - loss: 0.5569 [Training] 93/15958 [..............................] - ETA: 52:22  accuracy: 0.8125 - loss: 0.5598 [Training] 94/15958 [..............................] - ETA: 52:22  accuracy: 0.8125 - loss: 0.5504 [Training] 95/15958 [..............................] - ETA: 52:22  accuracy: 0.7708 - loss: 0.5846 [Training] 96/15958 [..............................] - ETA: 52:22  accuracy: 0.7292 - loss: 0.5702 [Training] 97/15958 [..............................] - ETA: 52:22  accuracy: 0.6875 - loss: 0.6048 [Training] 98/15958 [..............................] - ETA: 52:22  accuracy: 0.8333 - loss: 0.5634 [Training] 99/15958 [..............................] - ETA: 52:22  accuracy: 0.8333 - loss: 0.5629 [Training] 100/15958 [..............................] - ETA: 52:22  accuracy: 0.7083 - loss: 0.5877 [Training] 101/15958 [..............................] - ETA: 52:22  accuracy: 0.8958 - loss: 0.5600 
------------- train result --------------
label:toxic - auc: 0.5399
label:severe_toxic - auc: 0.4545
label:obscene - auc: 0.4668
label:threat - auc: 0.4300
label:insult - auc: 0.5460
label:identity_hate - auc: 0.6390
[Evaluating] 1/3990 [..............................] - ETA: 3:37[Evaluating] 2/3990 [..............................] - ETA: 3:46[Evaluating] 3/3990 [..............................] - ETA: 3:04[Evaluating] 4/3990 [..............................] - ETA: 3:13[Evaluating] 5/3990 [..............................] - ETA: 3:18[Evaluating] 6/3990 [..............................] - ETA: 3:17[Evaluating] 7/3990 [..............................] - ETA: 3:17[Evaluating] 8/3990 [..............................] - ETA: 3:16[Evaluating] 9/3990 [..............................] - ETA: 3:13[Evaluating] 10/3990 [..............................] - ETA: 3:16[Evaluating] 11/3990 [..............................] - ETA: 3:18[Evaluating] 12/3990 [..............................] - ETA: 3:19[Evaluating] 13/3990 [..............................] - ETA: 3:11[Evaluating] 14/3990 [..............................] - ETA: 3:13[Evaluating] 15/3990 [..............................] - ETA: 3:09[Evaluating] 16/3990 [..............................] - ETA: 3:01[Evaluating] 17/3990 [..............................] - ETA: 3:04[Evaluating] 18/3990 [..............................] - ETA: 2:59[Evaluating] 19/3990 [..............................] - ETA: 3:01[Evaluating] 20/3990 [..............................] - ETA: 3:02[Evaluating] 21/3990 [..............................] - ETA: 3:02[Evaluating] 22/3990 [..............................] - ETA: 3:04[Evaluating] 23/3990 [..............................] - ETA: 2:59[Evaluating] 24/3990 [..............................] - ETA: 3:00[Evaluating] 25/3990 [..............................] - ETA: 2:57[Evaluating] 26/3990 [..............................] - ETA: 2:58[Evaluating] 27/3990 [..............................] - ETA: 2:58[Evaluating] 28/3990 [..............................] - ETA: 2:56[Evaluating] 29/3990 [..............................] - ETA: 2:57[Evaluating] 30/3990 [..............................] - ETA: 2:55[Evaluating] 31/3990 [..............................] - ETA: 2:53[Evaluating] 32/3990 [..............................] - ETA: 2:50[Evaluating] 33/3990 [..............................] - ETA: 2:52[Evaluating] 34/3990 [..............................] - ETA: 2:49[Evaluating] 35/3990 [..............................] - ETA: 2:50[Evaluating] 36/3990 [..............................] - ETA: 2:49[Evaluating] 37/3990 [..............................] - ETA: 2:47[Evaluating] 38/3990 [..............................] - ETA: 2:48[Evaluating] 39/3990 [..............................] - ETA: 2:49[Evaluating] 40/3990 [..............................] - ETA: 2:46[Evaluating] 41/3990 [..............................] - ETA: 2:44[Evaluating] 42/3990 [..............................] - ETA: 2:44[Evaluating] 43/3990 [..............................] - ETA: 2:42[Evaluating] 44/3990 [..............................] - ETA: 2:42[Evaluating] 45/3990 [..............................] - ETA: 2:43[Evaluating] 46/3990 [..............................] - ETA: 2:43[Evaluating] 47/3990 [..............................] - ETA: 2:41[Evaluating] 48/3990 [..............................] - ETA: 2:41[Evaluating] 49/3990 [..............................] - ETA: 2:39[Evaluating] 50/3990 [..............................] - ETA: 2:39[Evaluating] 51/3990 [..............................] - ETA: 2:39[Evaluating] 52/3990 [..............................] - ETA: 2:39[Evaluating] 53/3990 [..............................] - ETA: 2:39[Evaluating] 54/3990 [..............................] - ETA: 2:40[Evaluating] 55/3990 [..............................] - ETA: 2:41[Evaluating] 56/3990 [..............................] - ETA: 2:40[Evaluating] 57/3990 [..............................] - ETA: 2:41[Evaluating] 58/3990 [..............................] - ETA: 2:42[Evaluating] 59/3990 [..............................] - ETA: 2:42[Evaluating] 60/3990 [..............................] - ETA: 2:43[Evaluating] 61/3990 [..............................] - ETA: 2:42[Evaluating] 62/3990 [..............................] - ETA: 2:43[Evaluating] 63/3990 [..............................] - ETA: 2:42[Evaluating] 64/3990 [..............................] - ETA: 2:41[Evaluating] 65/3990 [..............................] - ETA: 2:41[Evaluating] 66/3990 [..............................] - ETA: 2:41[Evaluating] 67/3990 [..............................] - ETA: 2:41[Evaluating] 68/3990 [..............................] - ETA: 2:42[Evaluating] 69/3990 [..............................] - ETA: 2:43[Evaluating] 70/3990 [..............................] - ETA: 2:43[Evaluating] 71/3990 [..............................] - ETA: 2:43[Evaluating] 72/3990 [..............................] - ETA: 2:44[Evaluating] 73/3990 [..............................] - ETA: 2:45[Evaluating] 74/3990 [..............................] - ETA: 2:45[Evaluating] 75/3990 [..............................] - ETA: 2:46[Evaluating] 76/3990 [..............................] - ETA: 2:46[Evaluating] 77/3990 [..............................] - ETA: 2:46[Evaluating] 78/3990 [..............................] - ETA: 2:47[Evaluating] 79/3990 [..............................] - ETA: 2:47[Evaluating] 80/3990 [..............................] - ETA: 2:47[Evaluating] 81/3990 [..............................] - ETA: 2:47[Evaluating] 82/3990 [..............................] - ETA: 2:46[Evaluating] 83/3990 [..............................] - ETA: 2:47[Evaluating] 84/3990 [..............................] - ETA: 2:46[Evaluating] 85/3990 [..............................] - ETA: 2:46[Evaluating] 86/3990 [..............................] - ETA: 2:46[Evaluating] 87/3990 [..............................] - ETA: 2:46[Evaluating] 88/3990 [..............................] - ETA: 2:46[Evaluating] 89/3990 [..............................] - ETA: 2:46[Evaluating] 90/3990 [..............................] - ETA: 2:45[Evaluating] 91/3990 [..............................] - ETA: 2:44[Evaluating] 92/3990 [..............................] - ETA: 2:44[Evaluating] 93/3990 [..............................] - ETA: 2:4505/28/2025 10:16:51 - INFO - root -   
Epoch: 1 -  loss: 0.6377 - auc: 0.5365 - valid_loss: 0.6040 - valid_auc: 0.5615 
05/28/2025 10:16:51 - INFO - root -   
Epoch 1: valid_loss improved from inf to 0.60400
05/28/2025 10:16:51 - INFO - transformers.configuration_utils -   Configuration saved in pybert/output/checkpoints/bert/config.json
05/28/2025 10:17:15 - INFO - transformers.modeling_utils -   Model weights saved in pybert/output/checkpoints/bert/pytorch_model.bin
[Evaluating] 94/3990 [..............................] - ETA: 2:45[Evaluating] 95/3990 [..............................] - ETA: 2:44[Evaluating] 96/3990 [..............................] - ETA: 2:45[Evaluating] 97/3990 [..............................] - ETA: 2:45[Evaluating] 98/3990 [..............................] - ETA: 2:44[Evaluating] 99/3990 [..............................] - ETA: 2:45[Evaluating] 100/3990 [..............................] - ETA: 2:45[Evaluating] 101/3990 [..............................] - ETA: 2:46------------- valid result --------------
label:toxic - auc: 0.5092
label:severe_toxic - auc: 0.4862
label:obscene - auc: 0.6337
label:threat - auc: 0.6808
label:insult - auc: 0.5764
label:identity_hate - auc: 0.6120
