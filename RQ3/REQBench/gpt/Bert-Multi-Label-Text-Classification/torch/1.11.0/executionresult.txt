05/28/2025 00:37:08 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='kaggle', do_data=False, do_lower_case=True, do_test=False, do_train=True, epochs=1, eval_batch_size=8, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='2', predict_checkpoints=0, resume_path='', save_best=True, seed=42, sorted=1, train_batch_size=8, train_max_seq_len=256, valid_size=0.2, warmup_proportion=0.1, weight_decay=0.01)
05/28/2025 00:37:08 - INFO - root -   Loading examples from cached file pybert/dataset/cached_train_examples_bert
05/28/2025 00:37:09 - INFO - root -   Loading features from cached file pybert/dataset/cached_train_features_256_bert
05/28/2025 00:37:15 - INFO - root -   sorted data by th length of input
05/28/2025 00:37:18 - INFO - root -   Loading examples from cached file pybert/dataset/cached_valid_examples_bert
05/28/2025 00:37:18 - INFO - root -   Loading features from cached file pybert/dataset/cached_valid_features_256_bert
05/28/2025 00:37:21 - INFO - root -   initializing model
05/28/2025 00:37:21 - INFO - transformers.configuration_utils -   loading configuration file pybert/pretrain/bert/base-uncased/config.json
05/28/2025 00:37:21 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 6,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

05/28/2025 00:37:21 - INFO - transformers.modeling_utils -   loading weights file pybert/pretrain/bert/base-uncased/pytorch_model.bin
05/28/2025 00:37:23 - INFO - transformers.modeling_utils -   Weights of BertForMultiLable not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2025 00:37:23 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
05/28/2025 00:37:23 - INFO - root -   initializing callbacks
05/28/2025 00:37:23 - INFO - root -   ***** Running training *****
05/28/2025 00:37:23 - INFO - root -     Num examples = 127657
05/28/2025 00:37:23 - INFO - root -     Num Epochs = 1
05/28/2025 00:37:23 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8
05/28/2025 00:37:23 - INFO - root -     Gradient Accumulation steps = 1
05/28/2025 00:37:23 - INFO - root -     Total optimization steps = 15958
05/28/2025 00:37:27 - INFO - root -   Epoch 1/1
[Training] 1/15958 [..............................] - ETA: 45:12  accuracy: 0.5625 - loss: 0.6664 [Training] 2/15958 [..............................] - ETA: 42:24  accuracy: 0.5625 - loss: 0.6796 [Training] 3/15958 [..............................] - ETA: 41:35  accuracy: 0.5208 - loss: 0.6737 [Training] 4/15958 [..............................] - ETA: 41:11  accuracy: 0.5833 - loss: 0.6782 [Training] 5/15958 [..............................] - ETA: 40:52  accuracy: 0.6458 - loss: 0.6671 [Training] 6/15958 [..............................] - ETA: 40:44  accuracy: 0.6458 - loss: 0.6645 [Training] 7/15958 [..............................] - ETA: 40:41  accuracy: 0.6042 - loss: 0.6716 [Training] 8/15958 [..............................] - ETA: 40:36  accuracy: 0.6250 - loss: 0.6694 [Training] 9/15958 [..............................] - ETA: 40:34  accuracy: 0.5625 - loss: 0.6744 [Training] 10/15958 [..............................] - ETA: 40:31  accuracy: 0.5625 - loss: 0.6764 [Training] 11/15958 [..............................] - ETA: 40:31  accuracy: 0.5625 - loss: 0.6586 [Training] 12/15958 [..............................] - ETA: 40:29  accuracy: 0.5625 - loss: 0.6785 [Training] 13/15958 [..............................] - ETA: 40:29  accuracy: 0.5833 - loss: 0.6767 [Training] 14/15958 [..............................] - ETA: 40:27  accuracy: 0.5833 - loss: 0.6727 [Training] 15/15958 [..............................] - ETA: 40:27  accuracy: 0.5208 - loss: 0.6940 [Training] 16/15958 [..............................] - ETA: 40:26  accuracy: 0.6042 - loss: 0.6651 [Training] 17/15958 [..............................] - ETA: 40:26  accuracy: 0.5833 - loss: 0.6632 [Training] 18/15958 [..............................] - ETA: 40:25  accuracy: 0.5625 - loss: 0.6470 [Training] 19/15958 [..............................] - ETA: 40:25  accuracy: 0.6667 - loss: 0.6680 [Training] 20/15958 [..............................] - ETA: 40:24  accuracy: 0.6042 - loss: 0.6906 [Training] 21/15958 [..............................] - ETA: 40:24  accuracy: 0.5833 - loss: 0.6560 [Training] 22/15958 [..............................] - ETA: 40:24  accuracy: 0.6042 - loss: 0.6573 [Training] 23/15958 [..............................] - ETA: 40:23  accuracy: 0.5417 - loss: 0.6820 [Training] 24/15958 [..............................] - ETA: 40:23  accuracy: 0.6042 - loss: 0.6663 [Training] 25/15958 [..............................] - ETA: 40:23  accuracy: 0.6042 - loss: 0.6830 [Training] 26/15958 [..............................] - ETA: 40:23  accuracy: 0.6042 - loss: 0.6660 [Training] 27/15958 [..............................] - ETA: 40:23  accuracy: 0.5208 - loss: 0.6576 [Training] 28/15958 [..............................] - ETA: 40:23  accuracy: 0.5417 - loss: 0.6982 [Training] 29/15958 [..............................] - ETA: 40:23  accuracy: 0.5833 - loss: 0.6806 [Training] 30/15958 [..............................] - ETA: 40:23  accuracy: 0.6458 - loss: 0.6661 [Training] 31/15958 [..............................] - ETA: 40:22  accuracy: 0.6667 - loss: 0.6575 [Training] 32/15958 [..............................] - ETA: 40:22  accuracy: 0.6250 - loss: 0.6638 [Training] 33/15958 [..............................] - ETA: 40:22  accuracy: 0.5625 - loss: 0.6726 [Training] 34/15958 [..............................] - ETA: 40:22  accuracy: 0.6042 - loss: 0.6574 [Training] 35/15958 [..............................] - ETA: 40:22  accuracy: 0.6458 - loss: 0.6626 [Training] 36/15958 [..............................] - ETA: 40:22  accuracy: 0.6458 - loss: 0.6499 [Training] 37/15958 [..............................] - ETA: 40:22  accuracy: 0.6042 - loss: 0.6491 [Training] 38/15958 [..............................] - ETA: 40:22  accuracy: 0.5625 - loss: 0.6623 [Training] 39/15958 [..............................] - ETA: 40:22  accuracy: 0.6250 - loss: 0.6490 [Training] 40/15958 [..............................] - ETA: 40:22  accuracy: 0.6042 - loss: 0.6579 [Training] 41/15958 [..............................] - ETA: 40:22  accuracy: 0.6042 - loss: 0.6498 [Training] 42/15958 [..............................] - ETA: 40:22  accuracy: 0.7083 - loss: 0.6438 [Training] 43/15958 [..............................] - ETA: 40:22  accuracy: 0.6250 - loss: 0.6612 [Training] 44/15958 [..............................] - ETA: 40:22  accuracy: 0.6250 - loss: 0.6456 [Training] 45/15958 [..............................] - ETA: 40:21  accuracy: 0.5833 - loss: 0.6572 [Training] 46/15958 [..............................] - ETA: 40:21  accuracy: 0.6042 - loss: 0.6467 [Training] 47/15958 [..............................] - ETA: 40:21  accuracy: 0.5625 - loss: 0.6554 [Training] 48/15958 [..............................] - ETA: 40:21  accuracy: 0.6250 - loss: 0.6391 [Training] 49/15958 [..............................] - ETA: 40:20  accuracy: 0.6042 - loss: 0.6544 [Training] 50/15958 [..............................] - ETA: 40:20  accuracy: 0.5625 - loss: 0.6703 [Training] 51/15958 [..............................] - ETA: 40:20  accuracy: 0.5833 - loss: 0.6609 [Training] 52/15958 [..............................] - ETA: 40:21  accuracy: 0.6458 - loss: 0.6414 [Training] 53/15958 [..............................] - ETA: 40:21  accuracy: 0.6250 - loss: 0.6545 [Training] 54/15958 [..............................] - ETA: 40:21  accuracy: 0.6250 - loss: 0.6497 [Training] 55/15958 [..............................] - ETA: 40:21  accuracy: 0.6667 - loss: 0.6393 [Training] 56/15958 [..............................] - ETA: 40:20  accuracy: 0.6667 - loss: 0.6302 [Training] 57/15958 [..............................] - ETA: 40:20  accuracy: 0.6042 - loss: 0.6361 [Training] 58/15958 [..............................] - ETA: 40:20  accuracy: 0.6250 - loss: 0.6301 [Training] 59/15958 [..............................] - ETA: 40:20  accuracy: 0.6250 - loss: 0.6203 [Training] 60/15958 [..............................] - ETA: 40:20  accuracy: 0.6458 - loss: 0.6241 [Training] 61/15958 [..............................] - ETA: 40:20  accuracy: 0.6458 - loss: 0.6549 [Training] 62/15958 [..............................] - ETA: 40:20  accuracy: 0.6667 - loss: 0.6289 [Training] 63/15958 [..............................] - ETA: 40:20  accuracy: 0.6667 - loss: 0.6297 [Training] 64/15958 [..............................] - ETA: 40:20  accuracy: 0.6250 - loss: 0.6356 [Training] 65/15958 [..............................] - ETA: 40:20  accuracy: 0.6250 - loss: 0.6196 [Training] 66/15958 [..............................] - ETA: 40:20  accuracy: 0.6875 - loss: 0.6094 [Training] 67/15958 [..............................] - ETA: 40:20  accuracy: 0.6667 - loss: 0.6155 [Training] 68/15958 [..............................] - ETA: 40:20  accuracy: 0.7292 - loss: 0.6177 [Training] 69/15958 [..............................] - ETA: 40:20  accuracy: 0.7500 - loss: 0.6123 [Training] 70/15958 [..............................] - ETA: 40:19  accuracy: 0.6458 - loss: 0.6151 [Training] 71/15958 [..............................] - ETA: 40:20  accuracy: 0.7292 - loss: 0.6136 [Training] 72/15958 [..............................] - ETA: 40:20  accuracy: 0.7083 - loss: 0.6262 [Training] 73/15958 [..............................] - ETA: 40:20  accuracy: 0.7500 - loss: 0.6253 [Training] 74/15958 [..............................] - ETA: 40:19  accuracy: 0.7500 - loss: 0.6097 [Training] 75/15958 [..............................] - ETA: 40:20  accuracy: 0.7083 - loss: 0.6045 [Training] 76/15958 [..............................] - ETA: 40:20  accuracy: 0.6875 - loss: 0.5986 [Training] 77/15958 [..............................] - ETA: 40:19  accuracy: 0.7708 - loss: 0.5905 [Training] 78/15958 [..............................] - ETA: 40:20  accuracy: 0.8125 - loss: 0.5981 [Training] 79/15958 [..............................] - ETA: 40:19  accuracy: 0.7083 - loss: 0.5904 [Training] 80/15958 [..............................] - ETA: 40:19  accuracy: 0.6875 - loss: 0.6009 [Training] 81/15958 [..............................] - ETA: 40:19  accuracy: 0.7083 - loss: 0.6055 [Training] 82/15958 [..............................] - ETA: 40:19  accuracy: 0.7292 - loss: 0.5939 [Training] 83/15958 [..............................] - ETA: 40:19  accuracy: 0.6875 - loss: 0.6041 [Training] 84/15958 [..............................] - ETA: 40:20  accuracy: 0.7083 - loss: 0.5976 [Training] 85/15958 [..............................] - ETA: 40:20  accuracy: 0.7292 - loss: 0.6041 [Training] 86/15958 [..............................] - ETA: 40:20  accuracy: 0.7083 - loss: 0.6128 [Training] 87/15958 [..............................] - ETA: 40:20  accuracy: 0.7708 - loss: 0.5928 [Training] 88/15958 [..............................] - ETA: 40:20  accuracy: 0.7917 - loss: 0.5666 [Training] 89/15958 [..............................] - ETA: 40:20  accuracy: 0.7708 - loss: 0.5846 [Training] 90/15958 [..............................] - ETA: 40:20  accuracy: 0.7500 - loss: 0.6012 [Training] 91/15958 [..............................] - ETA: 40:20  accuracy: 0.8125 - loss: 0.5849 [Training] 92/15958 [..............................] - ETA: 40:20  accuracy: 0.7292 - loss: 0.5766 [Training] 93/15958 [..............................] - ETA: 40:20  accuracy: 0.7917 - loss: 0.5641 [Training] 94/15958 [..............................] - ETA: 40:20  accuracy: 0.8333 - loss: 0.5605 [Training] 95/15958 [..............................] - ETA: 40:20  accuracy: 0.7083 - loss: 0.5926 [Training] 96/15958 [..............................] - ETA: 40:20  accuracy: 0.7917 - loss: 0.5621 [Training] 97/15958 [..............................] - ETA: 40:19  accuracy: 0.7708 - loss: 0.5865 [Training] 98/15958 [..............................] - ETA: 40:20  accuracy: 0.8333 - loss: 0.5505 [Training] 99/15958 [..............................] - ETA: 40:20  accuracy: 0.7708 - loss: 0.5648 [Training] 100/15958 [..............................] - ETA: 40:20  accuracy: 0.7708 - loss: 0.5835 [Training] 101/15958 [..............................] - ETA: 40:20  accuracy: 0.8333 - loss: 0.5577 
------------- train result --------------
label:toxic - auc: 0.5628
label:severe_toxic - auc: 0.6236
label:obscene - auc: 0.5525
label:threat - auc: 0.3408
label:insult - auc: 0.4853
label:identity_hate - auc: 0.5869
[Evaluating] 1/3990 [..............................] - ETA: 2:28[Evaluating] 2/3990 [..............................] - ETA: 2:47[Evaluating] 3/3990 [..............................] - ETA: 2:24[Evaluating] 4/3990 [..............................] - ETA: 2:33[Evaluating] 5/3990 [..............................] - ETA: 2:39[Evaluating] 6/3990 [..............................] - ETA: 2:42[Evaluating] 7/3990 [..............................] - ETA: 2:43[Evaluating] 8/3990 [..............................] - ETA: 2:42[Evaluating] 9/3990 [..............................] - ETA: 2:41[Evaluating] 10/3990 [..............................] - ETA: 2:43[Evaluating] 11/3990 [..............................] - ETA: 2:45[Evaluating] 12/3990 [..............................] - ETA: 2:47[Evaluating] 13/3990 [..............................] - ETA: 2:40[Evaluating] 14/3990 [..............................] - ETA: 2:41[Evaluating] 15/3990 [..............................] - ETA: 2:38[Evaluating] 16/3990 [..............................] - ETA: 2:33[Evaluating] 17/3990 [..............................] - ETA: 2:34[Evaluating] 18/3990 [..............................] - ETA: 2:30[Evaluating] 19/3990 [..............................] - ETA: 2:32[Evaluating] 20/3990 [..............................] - ETA: 2:33[Evaluating] 21/3990 [..............................] - ETA: 2:33[Evaluating] 22/3990 [..............................] - ETA: 2:35[Evaluating] 23/3990 [..............................] - ETA: 2:31[Evaluating] 24/3990 [..............................] - ETA: 2:32[Evaluating] 25/3990 [..............................] - ETA: 2:30[Evaluating] 26/3990 [..............................] - ETA: 2:31[Evaluating] 27/3990 [..............................] - ETA: 2:31[Evaluating] 28/3990 [..............................] - ETA: 2:29[Evaluating] 29/3990 [..............................] - ETA: 2:30[Evaluating] 30/3990 [..............................] - ETA: 2:29[Evaluating] 31/3990 [..............................] - ETA: 2:27[Evaluating] 32/3990 [..............................] - ETA: 2:24[Evaluating] 33/3990 [..............................] - ETA: 2:26[Evaluating] 34/3990 [..............................] - ETA: 2:23[Evaluating] 35/3990 [..............................] - ETA: 2:25[Evaluating] 36/3990 [..............................] - ETA: 2:23[Evaluating] 37/3990 [..............................] - ETA: 2:22[Evaluating] 38/3990 [..............................] - ETA: 2:23[Evaluating] 39/3990 [..............................] - ETA: 2:23[Evaluating] 40/3990 [..............................] - ETA: 2:21[Evaluating] 41/3990 [..............................] - ETA: 2:19[Evaluating] 42/3990 [..............................] - ETA: 2:19[Evaluating] 43/3990 [..............................] - ETA: 2:17[Evaluating] 44/3990 [..............................] - ETA: 2:18[Evaluating] 45/3990 [..............................] - ETA: 2:19[Evaluating] 46/3990 [..............................] - ETA: 2:18[Evaluating] 47/3990 [..............................] - ETA: 2:17[Evaluating] 48/3990 [..............................] - ETA: 2:16[Evaluating] 49/3990 [..............................] - ETA: 2:15[Evaluating] 50/3990 [..............................] - ETA: 2:15[Evaluating] 51/3990 [..............................] - ETA: 2:15[Evaluating] 52/3990 [..............................] - ETA: 2:14[Evaluating] 53/3990 [..............................] - ETA: 2:15[Evaluating] 54/3990 [..............................] - ETA: 2:16[Evaluating] 55/3990 [..............................] - ETA: 2:17[Evaluating] 56/3990 [..............................] - ETA: 2:16[Evaluating] 57/3990 [..............................] - ETA: 2:16[Evaluating] 58/3990 [..............................] - ETA: 2:17[Evaluating] 59/3990 [..............................] - ETA: 2:17[Evaluating] 60/3990 [..............................] - ETA: 2:18[Evaluating] 61/3990 [..............................] - ETA: 2:17[Evaluating] 62/3990 [..............................] - ETA: 2:18[Evaluating] 63/3990 [..............................] - ETA: 2:17[Evaluating] 64/3990 [..............................] - ETA: 2:17[Evaluating] 65/3990 [..............................] - ETA: 2:16[Evaluating] 66/3990 [..............................] - ETA: 2:16[Evaluating] 67/3990 [..............................] - ETA: 2:16[Evaluating] 68/3990 [..............................] - ETA: 2:17[Evaluating] 69/3990 [..............................] - ETA: 2:17[Evaluating] 70/3990 [..............................] - ETA: 2:18[Evaluating] 71/3990 [..............................] - ETA: 2:18[Evaluating] 72/3990 [..............................] - ETA: 2:19[Evaluating] 73/3990 [..............................] - ETA: 2:19[Evaluating] 74/3990 [..............................] - ETA: 2:20[Evaluating] 75/3990 [..............................] - ETA: 2:20[Evaluating] 76/3990 [..............................] - ETA: 2:20[Evaluating] 77/3990 [..............................] - ETA: 2:20[Evaluating] 78/3990 [..............................] - ETA: 2:21[Evaluating] 79/3990 [..............................] - ETA: 2:21[Evaluating] 80/3990 [..............................] - ETA: 2:21[Evaluating] 81/3990 [..............................] - ETA: 2:21[Evaluating] 82/3990 [..............................] - ETA: 2:20[Evaluating] 83/3990 [..............................] - ETA: 2:21[Evaluating] 84/3990 [..............................] - ETA: 2:20[Evaluating] 85/3990 [..............................] - ETA: 2:20[Evaluating] 86/3990 [..............................] - ETA: 2:20[Evaluating] 87/3990 [..............................] - ETA: 2:20[Evaluating] 88/3990 [..............................] - ETA: 2:21[Evaluating] 89/3990 [..............................] - ETA: 2:20[Evaluating] 90/3990 [..............................] - ETA: 2:19[Evaluating] 91/3990 [..............................] - ETA: 2:19[Evaluating] 92/3990 [..............................] - ETA: 2:19[Evaluating] 93/3990 [..............................] - ETA: 2:19[Evaluating] 94/3990 [..............................] - ETA: 2:1905/28/2025 00:37:46 - INFO - root -   
Epoch: 1 -  loss: 0.6355 - auc: 0.5479 - valid_loss: 0.6022 - valid_auc: 0.5627 
05/28/2025 00:37:49 - INFO - root -   
Epoch 1: valid_loss improved from inf to 0.60222
05/28/2025 00:37:49 - INFO - transformers.configuration_utils -   Configuration saved in pybert/output/checkpoints/bert/config.json
05/28/2025 00:38:02 - INFO - transformers.modeling_utils -   Model weights saved in pybert/output/checkpoints/bert/pytorch_model.bin
[Evaluating] 95/3990 [..............................] - ETA: 2:19[Evaluating] 96/3990 [..............................] - ETA: 2:19[Evaluating] 97/3990 [..............................] - ETA: 2:19[Evaluating] 98/3990 [..............................] - ETA: 2:19[Evaluating] 99/3990 [..............................] - ETA: 2:19[Evaluating] 100/3990 [..............................] - ETA: 2:19[Evaluating] 101/3990 [..............................] - ETA: 2:19------------- valid result --------------
label:toxic - auc: 0.5299
label:severe_toxic - auc: 0.4869
label:obscene - auc: 0.6401
label:threat - auc: 0.6760
label:insult - auc: 0.5820
label:identity_hate - auc: 0.6096
