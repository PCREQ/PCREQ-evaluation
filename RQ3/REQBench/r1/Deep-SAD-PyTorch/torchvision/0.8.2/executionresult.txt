INFO:root:Log file is ../log/DeepSAD/mnist_test/log.txt
INFO:root:Data path is ../data
INFO:root:Export path is ../log/DeepSAD/mnist_test
INFO:root:Dataset: mnist
INFO:root:Normal class: 0
INFO:root:Ratio of labeled normal train samples: 0.00
INFO:root:Ratio of labeled anomalous samples: 0.01
INFO:root:Pollution ratio of unlabeled train data: 0.10
INFO:root:Known anomaly class: 1
INFO:root:Network: mnist_LeNet
INFO:root:Eta-parameter: 1.00
INFO:root:Set seed to 2.
INFO:root:Computation device: cuda
INFO:root:Number of threads: 0
INFO:root:Number of dataloader workers: 0
INFO:root:Pretraining: True
INFO:root:Pretraining optimizer: adam
INFO:root:Pretraining learning rate: 0.0001
INFO:root:Pretraining epochs: 1
INFO:root:Pretraining learning rate scheduler milestones: ()
INFO:root:Pretraining batch size: 128
INFO:root:Pretraining weight decay: 0.0005
INFO:root:Starting pretraining...
/home/lei/anaconda3/envs/py37-4/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
INFO:root:| Epoch: 001/001 | Train Time: 0.675s | Train Loss: 0.262662 |
INFO:root:Pretraining Time: 0.675s
INFO:root:Finished pretraining.
INFO:root:Testing autoencoder...
INFO:root:Test Loss: 0.224842
INFO:root:Test AUC: 51.31%
INFO:root:Test Time: 0.705s
INFO:root:Finished testing autoencoder.
INFO:root:Training optimizer: adam
INFO:root:Training learning rate: 0.0001
INFO:root:Training epochs: 1
INFO:root:Training learning rate scheduler milestones: (50,)
INFO:root:Training batch size: 128
INFO:root:Training weight decay: 5e-07
INFO:root:Initializing center c...
INFO:root:Center c initialized.
INFO:root:Starting training...
/home/lei/anaconda3/envs/py37-4/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
INFO:root:| Epoch: 001/001 | Train Time: 0.619s | Train Loss: 3.381837 |
INFO:root:Training Time: 0.620s
INFO:root:Finished training.
INFO:root:Starting testing...
INFO:root:Test Loss: 1.564093
INFO:root:Test AUC: 81.77%
INFO:root:Test Time: 0.665s
INFO:root:Finished testing.
