SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  219.8441, NLL-Loss  219.8433, KL-Loss    0.3833, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  173.2238, NLL-Loss  173.1753, KL-Loss   22.2254, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  126.3954, NLL-Loss  126.3254, KL-Loss   28.3383, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  131.8163, NLL-Loss  131.6992, KL-Loss   41.8099, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  129.4995, NLL-Loss  129.3418, KL-Loss   49.7141, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  122.2887, NLL-Loss  122.0766, KL-Loss   59.0242, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss   95.4588, NLL-Loss   95.2215, KL-Loss   58.3089, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  126.8677, NLL-Loss  126.5727, KL-Loss   63.9987, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  116.5657, NLL-Loss  116.2072, KL-Loss   68.6622, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  134.7523, NLL-Loss  134.3648, KL-Loss   65.5525, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  123.4630, NLL-Loss  123.0161, KL-Loss   66.7634, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  130.1219, NLL-Loss  129.6121, KL-Loss   67.2731, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss  120.7694, NLL-Loss  120.2133, KL-Loss   64.8351, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss  125.2758, NLL-Loss  124.6140, KL-Loss   68.1753, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss  112.7880, NLL-Loss  112.0839, KL-Loss   64.0856, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss  113.9922, NLL-Loss  113.2124, KL-Loss   62.7258, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  127.9475, NLL-Loss  127.0888, KL-Loss   61.0560, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  114.1528, NLL-Loss  113.2262, KL-Loss   58.2550, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss  110.6177, NLL-Loss  109.5462, KL-Loss   59.5746, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss   89.3249, NLL-Loss   88.1086, KL-Loss   59.8196, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss  104.8159, NLL-Loss  103.6214, KL-Loss   51.9834, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  120.9834, NLL-Loss  119.5787, KL-Loss   54.1173, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  108.8911, NLL-Loss  107.4121, KL-Loss   50.4597, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss  108.1595, NLL-Loss  106.4749, KL-Loss   50.9178, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss  101.9664, NLL-Loss  100.1602, KL-Loss   48.3873, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss  113.4583, NLL-Loss  111.3493, KL-Loss   50.1093, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss  101.3889, NLL-Loss   99.3205, KL-Loss   43.6115, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss  100.5430, NLL-Loss   98.3722, KL-Loss   44.2723, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  119.4181
Model saved at bin/2024-Dec-17-18:23:40/E0.pytorch
VALID Batch 0000/105, Loss  124.9866, NLL-Loss  122.8466, KL-Loss   43.5398, KL-Weight  0.049
VALID Batch 0050/105, Loss  124.9947, NLL-Loss  122.6766, KL-Loss   47.1640, KL-Weight  0.049
VALID Batch 0100/105, Loss   90.2626, NLL-Loss   88.2973, KL-Loss   39.9874, KL-Weight  0.049
VALID Batch 0105/105, Loss   91.3505, NLL-Loss   89.0412, KL-Loss   46.9852, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  105.0438
