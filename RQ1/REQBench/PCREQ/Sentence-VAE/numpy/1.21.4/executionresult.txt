SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  221.1954, NLL-Loss  221.1947, KL-Loss    0.3606, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  129.9720, NLL-Loss  129.9481, KL-Loss   10.9754, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  164.6562, NLL-Loss  164.5862, KL-Loss   28.2873, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  131.2959, NLL-Loss  131.1889, KL-Loss   38.1796, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  129.2181, NLL-Loss  129.0698, KL-Loss   46.7434, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  130.0402, NLL-Loss  129.8534, KL-Loss   51.9776, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss  114.1492, NLL-Loss  113.9310, KL-Loss   53.5943, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  128.0440, NLL-Loss  127.7659, KL-Loss   60.3473, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  103.1968, NLL-Loss  102.8601, KL-Loss   64.5034, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  120.9882, NLL-Loss  120.6191, KL-Loss   62.4361, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  128.4074, NLL-Loss  127.9540, KL-Loss   67.7376, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  110.5743, NLL-Loss  110.0376, KL-Loss   70.8293, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss  120.0893, NLL-Loss  119.5438, KL-Loss   63.6034, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss  101.9559, NLL-Loss  101.3089, KL-Loss   66.6461, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss  118.7826, NLL-Loss  118.0685, KL-Loss   64.9956, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss  106.7135, NLL-Loss  105.9919, KL-Loss   58.0442, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  117.8670, NLL-Loss  116.9941, KL-Loss   62.0637, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  100.8736, NLL-Loss  100.0033, KL-Loss   54.7163, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss   90.6857, NLL-Loss   89.7113, KL-Loss   54.1748, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss  100.9896, NLL-Loss   99.8874, KL-Loss   54.2081, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss   96.8624, NLL-Loss   95.6445, KL-Loss   53.0038, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  110.6664, NLL-Loss  109.2255, KL-Loss   55.5091, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  119.1195, NLL-Loss  117.5802, KL-Loss   52.5157, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss  136.1215, NLL-Loss  134.4570, KL-Loss   50.3101, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss  100.5804, NLL-Loss   98.8420, KL-Loss   46.5699, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss  101.9354, NLL-Loss  100.0440, KL-Loss   44.9402, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss  112.8787, NLL-Loss  110.8379, KL-Loss   43.0320, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss  105.3175, NLL-Loss  103.1830, KL-Loss   43.5331, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  119.6604
Model saved at bin/2025-Jan-09-13:33:46/E0.pytorch
VALID Batch 0000/105, Loss  123.7566, NLL-Loss  121.7988, KL-Loss   39.8334, KL-Weight  0.049
VALID Batch 0050/105, Loss  123.8970, NLL-Loss  121.6573, KL-Loss   45.5690, KL-Weight  0.049
VALID Batch 0100/105, Loss   90.9857, NLL-Loss   89.0043, KL-Loss   40.3148, KL-Weight  0.049
VALID Batch 0105/105, Loss   90.8153, NLL-Loss   88.6574, KL-Loss   43.9065, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  104.5391
