01/11/2025 07:16:58 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='kaggle', do_data=False, do_lower_case=True, do_test=False, do_train=True, epochs=1, eval_batch_size=8, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='0', predict_checkpoints=0, resume_path='', save_best=True, seed=42, sorted=1, train_batch_size=8, train_max_seq_len=256, valid_size=0.2, warmup_proportion=0.1, weight_decay=0.01)
01/11/2025 07:16:59 - INFO - root -   Loading examples from cached file pybert/dataset/cached_train_examples_bert
01/11/2025 07:16:59 - INFO - root -   Loading features from cached file pybert/dataset/cached_train_features_256_bert
01/11/2025 07:17:04 - INFO - root -   sorted data by th length of input
01/11/2025 07:17:06 - INFO - root -   Loading examples from cached file pybert/dataset/cached_valid_examples_bert
01/11/2025 07:17:06 - INFO - root -   Loading features from cached file pybert/dataset/cached_valid_features_256_bert
01/11/2025 07:17:07 - INFO - root -   initializing model
Some weights of the model checkpoint at pybert/pretrain/bert/base-uncased were not used when initializing BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForMultiLable from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMultiLable from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForMultiLable were not initialized from the model checkpoint at pybert/pretrain/bert/base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/11/2025 07:17:10 - INFO - root -   initializing callbacks
01/11/2025 07:17:10 - INFO - root -   ***** Running training *****
01/11/2025 07:17:10 - INFO - root -     Num examples = 127657
01/11/2025 07:17:10 - INFO - root -     Num Epochs = 1
01/11/2025 07:17:10 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8
01/11/2025 07:17:10 - INFO - root -     Gradient Accumulation steps = 1
01/11/2025 07:17:10 - INFO - root -     Total optimization steps = 15958
01/11/2025 07:17:12 - INFO - root -   Epoch 1/1
[Training] 1/15958 [..............................] - ETA: 1:01:10  accuracy: 0.4792 - loss: 0.6911 [Training] 2/15958 [..............................] - ETA: 56:52  accuracy: 0.5208 - loss: 0.6710 [Training] 3/15958 [..............................] - ETA: 53:19  accuracy: 0.5000 - loss: 0.6836 [Training] 4/15958 [..............................] - ETA: 52:21  accuracy: 0.5417 - loss: 0.6784 [Training] 5/15958 [..............................] - ETA: 51:41  accuracy: 0.5625 - loss: 0.6801 [Training] 6/15958 [..............................] - ETA: 51:24  accuracy: 0.4583 - loss: 0.6917 [Training] 7/15958 [..............................] - ETA: 51:22  accuracy: 0.6458 - loss: 0.6640 [Training] 8/15958 [..............................] - ETA: 51:10  accuracy: 0.5417 - loss: 0.6752 [Training] 9/15958 [..............................] - ETA: 51:06  accuracy: 0.6250 - loss: 0.6700 [Training] 10/15958 [..............................] - ETA: 51:02  accuracy: 0.5833 - loss: 0.6789 [Training] 11/15958 [..............................] - ETA: 50:55  accuracy: 0.5000 - loss: 0.6778 [Training] 12/15958 [..............................] - ETA: 50:55  accuracy: 0.5833 - loss: 0.6766 [Training] 13/15958 [..............................] - ETA: 50:53  accuracy: 0.5625 - loss: 0.6661 [Training] 14/15958 [..............................] - ETA: 50:48  accuracy: 0.5625 - loss: 0.6846 [Training] 15/15958 [..............................] - ETA: 50:36  accuracy: 0.4583 - loss: 0.7024 [Training] 16/15958 [..............................] - ETA: 50:19  accuracy: 0.5000 - loss: 0.6848 [Training] 17/15958 [..............................] - ETA: 50:06  accuracy: 0.5625 - loss: 0.6729 [Training] 18/15958 [..............................] - ETA: 49:56  accuracy: 0.5625 - loss: 0.6569 [Training] 19/15958 [..............................] - ETA: 49:45  accuracy: 0.5625 - loss: 0.6694 [Training] 20/15958 [..............................] - ETA: 49:35  accuracy: 0.5208 - loss: 0.6982 [Training] 21/15958 [..............................] - ETA: 49:25  accuracy: 0.5833 - loss: 0.6670 [Training] 22/15958 [..............................] - ETA: 49:16  accuracy: 0.6042 - loss: 0.6647 [Training] 23/15958 [..............................] - ETA: 49:08  accuracy: 0.4792 - loss: 0.6767 [Training] 24/15958 [..............................] - ETA: 49:00  accuracy: 0.5625 - loss: 0.6673 [Training] 25/15958 [..............................] - ETA: 48:53  accuracy: 0.6250 - loss: 0.6717 [Training] 26/15958 [..............................] - ETA: 48:46  accuracy: 0.6250 - loss: 0.6616 [Training] 27/15958 [..............................] - ETA: 48:40  accuracy: 0.6042 - loss: 0.6595 [Training] 28/15958 [..............................] - ETA: 48:35  accuracy: 0.5625 - loss: 0.6905 [Training] 29/15958 [..............................] - ETA: 48:29  accuracy: 0.5417 - loss: 0.6871 [Training] 30/15958 [..............................] - ETA: 48:25  accuracy: 0.5417 - loss: 0.6748 [Training] 31/15958 [..............................] - ETA: 48:20  accuracy: 0.5208 - loss: 0.6834 [Training] 32/15958 [..............................] - ETA: 48:15  accuracy: 0.5417 - loss: 0.6609 [Training] 33/15958 [..............................] - ETA: 48:11  accuracy: 0.5625 - loss: 0.6705 [Training] 34/15958 [..............................] - ETA: 48:07  accuracy: 0.5625 - loss: 0.6789 [Training] 35/15958 [..............................] - ETA: 48:04  accuracy: 0.5625 - loss: 0.6476 [Training] 36/15958 [..............................] - ETA: 48:00  accuracy: 0.6250 - loss: 0.6586 [Training] 37/15958 [..............................] - ETA: 47:57  accuracy: 0.6250 - loss: 0.6465 [Training] 38/15958 [..............................] - ETA: 47:54  accuracy: 0.5833 - loss: 0.6559 [Training] 39/15958 [..............................] - ETA: 47:51  accuracy: 0.6458 - loss: 0.6475 [Training] 40/15958 [..............................] - ETA: 47:48  accuracy: 0.5625 - loss: 0.6533 [Training] 41/15958 [..............................] - ETA: 47:46  accuracy: 0.6042 - loss: 0.6595 [Training] 42/15958 [..............................] - ETA: 47:43  accuracy: 0.7083 - loss: 0.6350 [Training] 43/15958 [..............................] - ETA: 47:41  accuracy: 0.5833 - loss: 0.6697 [Training] 44/15958 [..............................] - ETA: 47:39  accuracy: 0.5625 - loss: 0.6467 [Training] 45/15958 [..............................] - ETA: 47:37  accuracy: 0.6458 - loss: 0.6489 [Training] 46/15958 [..............................] - ETA: 47:35  accuracy: 0.6458 - loss: 0.6527 [Training] 47/15958 [..............................] - ETA: 47:33  accuracy: 0.6458 - loss: 0.6483 [Training] 48/15958 [..............................] - ETA: 47:31  accuracy: 0.6250 - loss: 0.6472 [Training] 49/15958 [..............................] - ETA: 47:30  accuracy: 0.5417 - loss: 0.6672 [Training] 50/15958 [..............................] - ETA: 47:28  accuracy: 0.5208 - loss: 0.6689 [Training] 51/15958 [..............................] - ETA: 47:26  accuracy: 0.6667 - loss: 0.6361 [Training] 52/15958 [..............................] - ETA: 47:25  accuracy: 0.6667 - loss: 0.6354 [Training] 53/15958 [..............................] - ETA: 47:23  accuracy: 0.5417 - loss: 0.6539 [Training] 54/15958 [..............................] - ETA: 47:21  accuracy: 0.5833 - loss: 0.6611 [Training] 55/15958 [..............................] - ETA: 47:20  accuracy: 0.5625 - loss: 0.6449 [Training] 56/15958 [..............................] - ETA: 47:19  accuracy: 0.6042 - loss: 0.6497 [Training] 57/15958 [..............................] - ETA: 47:17  accuracy: 0.6667 - loss: 0.6354 [Training] 58/15958 [..............................] - ETA: 47:16  accuracy: 0.6250 - loss: 0.6467 [Training] 59/15958 [..............................] - ETA: 47:15  accuracy: 0.7292 - loss: 0.6158 [Training] 60/15958 [..............................] - ETA: 47:14  accuracy: 0.6667 - loss: 0.6296 [Training] 61/15958 [..............................] - ETA: 47:13  accuracy: 0.6875 - loss: 0.6571 [Training] 62/15958 [..............................] - ETA: 47:11  accuracy: 0.7292 - loss: 0.6263 [Training] 63/15958 [..............................] - ETA: 47:10  accuracy: 0.7500 - loss: 0.6117 [Training] 64/15958 [..............................] - ETA: 47:09  accuracy: 0.6667 - loss: 0.6190 [Training] 65/15958 [..............................] - ETA: 47:08  accuracy: 0.6250 - loss: 0.6247 [Training] 66/15958 [..............................] - ETA: 47:07  accuracy: 0.7708 - loss: 0.6198 [Training] 67/15958 [..............................] - ETA: 47:06  accuracy: 0.7292 - loss: 0.6129 [Training] 68/15958 [..............................] - ETA: 47:05  accuracy: 0.6667 - loss: 0.6170 [Training] 69/15958 [..............................] - ETA: 47:04  accuracy: 0.6875 - loss: 0.6304 [Training] 70/15958 [..............................] - ETA: 47:03  accuracy: 0.6250 - loss: 0.6181 [Training] 71/15958 [..............................] - ETA: 47:02  accuracy: 0.7500 - loss: 0.6146 [Training] 72/15958 [..............................] - ETA: 47:01  accuracy: 0.6667 - loss: 0.6377 [Training] 73/15958 [..............................] - ETA: 47:00  accuracy: 0.7500 - loss: 0.6322 [Training] 74/15958 [..............................] - ETA: 46:59  accuracy: 0.6875 - loss: 0.6183 [Training] 75/15958 [..............................] - ETA: 46:59  accuracy: 0.7500 - loss: 0.5930 [Training] 76/15958 [..............................] - ETA: 46:58  accuracy: 0.7083 - loss: 0.6059 [Training] 77/15958 [..............................] - ETA: 46:57  accuracy: 0.7708 - loss: 0.5893 [Training] 78/15958 [..............................] - ETA: 46:56  accuracy: 0.6875 - loss: 0.6119 [Training] 79/15958 [..............................] - ETA: 46:55  accuracy: 0.7292 - loss: 0.5895 [Training] 80/15958 [..............................] - ETA: 46:55  accuracy: 0.8125 - loss: 0.5830 [Training] 81/15958 [..............................] - ETA: 46:54  accuracy: 0.6667 - loss: 0.6097 [Training] 82/15958 [..............................] - ETA: 46:53  accuracy: 0.7292 - loss: 0.6086 [Training] 83/15958 [..............................] - ETA: 46:53  accuracy: 0.6667 - loss: 0.6141 [Training] 84/15958 [..............................] - ETA: 46:52  accuracy: 0.6875 - loss: 0.5879 [Training] 85/15958 [..............................] - ETA: 46:51  accuracy: 0.7083 - loss: 0.6097 [Training] 86/15958 [..............................] - ETA: 46:51  accuracy: 0.7500 - loss: 0.5940 [Training] 87/15958 [..............................] - ETA: 46:50  accuracy: 0.6667 - loss: 0.6106 [Training] 88/15958 [..............................] - ETA: 46:50  accuracy: 0.8333 - loss: 0.5729 [Training] 89/15958 [..............................] - ETA: 46:49  accuracy: 0.7500 - loss: 0.5690 [Training] 90/15958 [..............................] - ETA: 46:48  accuracy: 0.8333 - loss: 0.5995 [Training] 91/15958 [..............................] - ETA: 46:48  accuracy: 0.7917 - loss: 0.5678 [Training] 92/15958 [..............................] - ETA: 46:47  accuracy: 0.8542 - loss: 0.5569 [Training] 93/15958 [..............................] - ETA: 46:47  accuracy: 0.8125 - loss: 0.5598 [Training] 94/15958 [..............................] - ETA: 46:46  accuracy: 0.8125 - loss: 0.5504 [Training] 95/15958 [..............................] - ETA: 46:46  accuracy: 0.7708 - loss: 0.5846 [Training] 96/15958 [..............................] - ETA: 46:45  accuracy: 0.7292 - loss: 0.5702 [Training] 97/15958 [..............................] - ETA: 46:45  accuracy: 0.6875 - loss: 0.6048 [Training] 98/15958 [..............................] - ETA: 46:44  accuracy: 0.8333 - loss: 0.5634 [Training] 99/15958 [..............................] - ETA: 46:44  accuracy: 0.8333 - loss: 0.5629 [Training] 100/15958 [..............................] - ETA: 46:43  accuracy: 0.7083 - loss: 0.5877 [Training] 101/15958 [..............................] - ETA: 46:43  accuracy: 0.8958 - loss: 0.5600 
------------- train result --------------
label:toxic - auc: 0.5399
label:severe_toxic - auc: 0.4545
label:obscene - auc: 0.4668
label:threat - auc: 0.4300
label:insult - auc: 0.5460
label:identity_hate - auc: 0.6390
[Evaluating] 1/3990 [..............................] - ETA: 2:48[Evaluating] 2/3990 [..............................] - ETA: 3:11[Evaluating] 3/3990 [..............................] - ETA: 2:40[Evaluating] 4/3990 [..............................] - ETA: 2:53[Evaluating] 5/3990 [..............................] - ETA: 3:00[Evaluating] 6/3990 [..............................] - ETA: 3:01[Evaluating] 7/3990 [..............................] - ETA: 3:02[Evaluating] 8/3990 [..............................] - ETA: 3:03[Evaluating] 9/3990 [..............................] - ETA: 3:01[Evaluating] 10/3990 [..............................] - ETA: 3:03[Evaluating] 11/3990 [..............................] - ETA: 3:06[Evaluating] 12/3990 [..............................] - ETA: 3:08[Evaluating] 13/3990 [..............................] - ETA: 3:00[Evaluating] 14/3990 [..............................] - ETA: 3:02[Evaluating] 15/3990 [..............................] - ETA: 2:58[Evaluating] 16/3990 [..............................] - ETA: 2:51[Evaluating] 17/3990 [..............................] - ETA: 2:54[Evaluating] 18/3990 [..............................] - ETA: 2:49[Evaluating] 19/3990 [..............................] - ETA: 2:51[Evaluating] 20/3990 [..............................] - ETA: 2:52[Evaluating] 21/3990 [..............................] - ETA: 2:52[Evaluating] 22/3990 [..............................] - ETA: 2:54[Evaluating] 23/3990 [..............................] - ETA: 2:50[Evaluating] 24/3990 [..............................] - ETA: 2:51[Evaluating] 25/3990 [..............................] - ETA: 2:48[Evaluating] 26/3990 [..............................] - ETA: 2:49[Evaluating] 27/3990 [..............................] - ETA: 2:50[Evaluating] 28/3990 [..............................] - ETA: 2:47[Evaluating] 29/3990 [..............................] - ETA: 2:48[Evaluating] 30/3990 [..............................] - ETA: 2:47[Evaluating] 31/3990 [..............................] - ETA: 2:45[Evaluating] 32/3990 [..............................] - ETA: 2:42[Evaluating] 33/3990 [..............................] - ETA: 2:43[Evaluating] 34/3990 [..............................] - ETA: 2:41[Evaluating] 35/3990 [..............................] - ETA: 2:42[Evaluating] 36/3990 [..............................] - ETA: 2:41[Evaluating] 37/3990 [..............................] - ETA: 2:38[Evaluating] 38/3990 [..............................] - ETA: 2:40[Evaluating] 39/3990 [..............................] - ETA: 2:40[Evaluating] 40/3990 [..............................] - ETA: 2:38[Evaluating] 41/3990 [..............................] - ETA: 2:35[Evaluating] 42/3990 [..............................] - ETA: 2:36[Evaluating] 43/3990 [..............................] - ETA: 2:33[Evaluating] 44/3990 [..............................] - ETA: 2:34[Evaluating] 45/3990 [..............................] - ETA: 2:35[Evaluating] 46/3990 [..............................] - ETA: 2:35[Evaluating] 47/3990 [..............................] - ETA: 2:33[Evaluating] 48/3990 [..............................] - ETA: 2:33[Evaluating] 49/3990 [..............................] - ETA: 2:31[Evaluating] 50/3990 [..............................] - ETA: 2:31[Evaluating] 51/3990 [..............................] - ETA: 2:31[Evaluating] 52/3990 [..............................] - ETA: 2:30[Evaluating] 53/3990 [..............................] - ETA: 2:31[Evaluating] 54/3990 [..............................] - ETA: 2:32[Evaluating] 55/3990 [..............................] - ETA: 2:33[Evaluating] 56/3990 [..............................] - ETA: 2:32[Evaluating] 57/3990 [..............................] - ETA: 2:33[Evaluating] 58/3990 [..............................] - ETA: 2:34[Evaluating] 59/3990 [..............................] - ETA: 2:33[Evaluating] 60/3990 [..............................] - ETA: 2:34[Evaluating] 61/3990 [..............................] - ETA: 2:34[Evaluating] 62/3990 [..............................] - ETA: 2:35[Evaluating] 63/3990 [..............................] - ETA: 2:34[Evaluating] 64/3990 [..............................] - ETA: 2:33[Evaluating] 65/3990 [..............................] - ETA: 2:33[Evaluating] 66/3990 [..............................] - ETA: 2:33[Evaluating] 67/3990 [..............................] - ETA: 2:33[Evaluating] 68/3990 [..............................] - ETA: 2:34[Evaluating] 69/3990 [..............................] - ETA: 2:34[Evaluating] 70/3990 [..............................] - ETA: 2:35[Evaluating] 71/3990 [..............................] - ETA: 2:35[Evaluating] 72/3990 [..............................] - ETA: 2:36[Evaluating] 73/3990 [..............................] - ETA: 2:36[Evaluating] 74/3990 [..............................] - ETA: 2:37[Evaluating] 75/3990 [..............................] - ETA: 2:37[Evaluating] 76/3990 [..............................] - ETA: 2:37[Evaluating] 77/3990 [..............................] - ETA: 2:38[Evaluating] 78/3990 [..............................] - ETA: 2:38[Evaluating] 79/3990 [..............................] - ETA: 2:39[Evaluating] 80/3990 [..............................] - ETA: 2:38[Evaluating] 81/3990 [..............................] - ETA: 2:38[Evaluating] 82/3990 [..............................] - ETA: 2:38[Evaluating] 83/3990 [..............................] - ETA: 2:38[Evaluating] 84/3990 [..............................] - ETA: 2:37[Evaluating] 85/3990 [..............................] - ETA: 2:38[Evaluating] 86/3990 [..............................] - ETA: 2:37[Evaluating] 87/3990 [..............................] - ETA: 2:38[Evaluating] 88/3990 [..............................] - ETA: 2:38[Evaluating] 89/3990 [..............................] - ETA: 2:38[Evaluating] 90/3990 [..............................] - ETA: 2:37[Evaluating] 91/3990 [..............................] - ETA: 2:36[Evaluating] 92/3990 [..............................] - ETA: 2:36[Evaluating] 93/3990 [..............................] - ETA: 2:3601/11/2025 07:17:34 - INFO - root -   
Epoch: 1 -  loss: 0.6377 - auc: 0.5365 - valid_loss: 0.6040 - valid_auc: 0.5615 
01/11/2025 07:17:34 - INFO - root -   
Epoch 1: valid_loss improved from inf to 0.60400
[Evaluating] 94/3990 [..............................] - ETA: 2:37[Evaluating] 95/3990 [..............................] - ETA: 2:36[Evaluating] 96/3990 [..............................] - ETA: 2:36[Evaluating] 97/3990 [..............................] - ETA: 2:37[Evaluating] 98/3990 [..............................] - ETA: 2:36[Evaluating] 99/3990 [..............................] - ETA: 2:36[Evaluating] 100/3990 [..............................] - ETA: 2:37[Evaluating] 101/3990 [..............................] - ETA: 2:37------------- valid result --------------
label:toxic - auc: 0.5092
label:severe_toxic - auc: 0.4862
label:obscene - auc: 0.6337
label:threat - auc: 0.6808
label:insult - auc: 0.5764
label:identity_hate - auc: 0.6120
